Below is a detailed breakdown of different methods to introduce uncertainty into a supervised machine learning model using an algorithm like Gradient Boosting, along with Python implementations for each method.

1. Ensemble of Gradient Boosting Models

Method: Train multiple Gradient Boosting models on bootstrapped subsets of the data. The variance in predictions across models reflects uncertainty.

Quantification:
	•	Mean Prediction: Average predictions across ensemble models.
	•	Uncertainty: Variance or standard deviation of predictions.

Code Implementation

from sklearn.utils import resample
from xgboost import XGBRegressor
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Generate synthetic data
np.random.seed(42)
X = np.random.rand(1000, 5)
y = np.sin(X[:, 0]) + np.cos(X[:, 1]) + 0.1 * np.random.randn(1000)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train ensemble models
n_models = 5
ensemble_preds = []

for i in range(n_models):
    # Bootstrap sampling
    X_resampled, y_resampled = resample(X_train, y_train, random_state=i)
    model = XGBRegressor(n_estimators=100, max_depth=3, random_state=i)
    model.fit(X_resampled, y_resampled)
    preds = model.predict(X_test)
    ensemble_preds.append(preds)

# Convert predictions to numpy array
ensemble_preds = np.array(ensemble_preds)

# Compute mean prediction and uncertainty
mean_prediction = ensemble_preds.mean(axis=0)
uncertainty = ensemble_preds.std(axis=0)

# Evaluate model
mse = mean_squared_error(y_test, mean_prediction)
print(f"Mean Squared Error: {mse}")
print(f"Sample Uncertainty (Standard Deviation): {uncertainty[:5]}")

2. Quantile Regression with Gradient Boosting

Method: Train separate models to predict the lower and upper quantiles, enabling computation of prediction intervals.

Quantification:
	•	Prediction Interval:
￼
	•	Uncertainty: Width of the interval.

Code Implementation

import lightgbm as lgb
from sklearn.metrics import mean_absolute_error

# Train quantile models
def train_quantile_model(X_train, y_train, quantile):
    params = {
        "objective": "quantile",
        "alpha": quantile,
        "boosting_type": "gbdt",
        "n_estimators": 100,
        "learning_rate": 0.05,
        "max_depth": 3,
        "random_state": 42,
    }
    model = lgb.LGBMRegressor(**params)
    model.fit(X_train, y_train)
    return model

# Train models for lower and upper quantiles
lower_quantile = 0.1  # 10th percentile
upper_quantile = 0.9  # 90th percentile

lower_model = train_quantile_model(X_train, y_train, lower_quantile)
upper_model = train_quantile_model(X_train, y_train, upper_quantile)

# Mean regression model
mean_model = lgb.LGBMRegressor(n_estimators=100, learning_rate=0.05, max_depth=3, random_state=42)
mean_model.fit(X_train, y_train)

# Predictions
y_pred_mean = mean_model.predict(X_test)
y_pred_lower = lower_model.predict(X_test)
y_pred_upper = upper_model.predict(X_test)

# Evaluate and calculate intervals
mae = mean_absolute_error(y_test, y_pred_mean)
print(f"Mean Absolute Error: {mae}")
print(f"Sample Prediction Intervals: [{y_pred_lower[:5]}, {y_pred_upper[:5]}]")

3. Monte Carlo Dropout

Method: Use dropout during both training and inference to introduce stochasticity into the model’s predictions.

Quantification:
	•	Mean Prediction: Average predictions from multiple forward passes.
	•	Uncertainty: Variance of predictions across passes.

Code Implementation

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

# Define a neural network with dropout
def build_model(input_dim):
    model = Sequential([
        Dense(64, activation='relu', input_dim=input_dim),
        Dropout(0.2),  # Dropout introduces stochasticity
        Dense(64, activation='relu'),
        Dropout(0.2),
        Dense(1)  # Regression output
    ])
    model.compile(optimizer='adam', loss='mse')
    return model

# Build and train the model
model = build_model(X_train.shape[1])
model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)

# Monte Carlo predictions
n_iter = 100
mc_preds = np.array([model.predict(X_test, verbose=0) for _ in range(n_iter)])

# Compute mean and uncertainty
mean_prediction = mc_preds.mean(axis=0)
uncertainty = mc_preds.std(axis=0)

print(f"Sample Mean Predictions: {mean_prediction[:5].flatten()}")
print(f"Sample Uncertainty (Standard Deviation): {uncertainty[:5].flatten()}")

4. Bootstrapping

Method: Generate bootstrapped datasets and train models on these resampled datasets. Use the variance in predictions to capture uncertainty.

Quantification:
	•	Mean Prediction: Average predictions across bootstrap samples.
	•	Uncertainty: Prediction variance or confidence intervals.

Code Implementation

# Bootstrapping (reusing the resampling method above)
bootstrap_preds = []

for i in range(n_models):
    X_resampled, y_resampled = resample(X_train, y_train, random_state=i)
    model = XGBRegressor(n_estimators=100, max_depth=3, random_state=i)
    model.fit(X_resampled, y_resampled)
    preds = model.predict(X_test)
    bootstrap_preds.append(preds)

# Convert to numpy array
bootstrap_preds = np.array(bootstrap_preds)

# Compute mean and confidence intervals
mean_prediction = bootstrap_preds.mean(axis=0)
lower_bound = np.percentile(bootstrap_preds, 5, axis=0)  # 5th percentile
upper_bound = np.percentile(bootstrap_preds, 95, axis=0)  # 95th percentile

print(f"Sample Mean Predictions: {mean_prediction[:5]}")
print(f"Sample Confidence Intervals: [{lower_bound[:5]}, {upper_bound[:5]}]")

5. Heteroscedastic Regression

Method: Predict both the target value (mean) and its variance, allowing the model to dynamically adapt uncertainty estimates.

Quantification:
	•	Variance Prediction: Represents uncertainty directly.

Code Implementation

# Define a neural network for heteroscedastic regression
def build_heteroscedastic_model(input_dim):
    inputs = tf.keras.Input(shape=(input_dim,))
    x = Dense(64, activation="relu")(inputs)
    x = Dense(64, activation="relu")(x)
    
    # Mean prediction
    mean_output = Dense(1, name="mean")(x)
    
    # Variance prediction
    variance_output = Dense(1, activation="softplus", name="variance")(x)
    
    model = tf.keras.Model(inputs, [mean_output, variance_output])
    return model

# Custom loss function
def heteroscedastic_loss(y_true, outputs):
    mean, variance = outputs
    precision = 1.0 / (variance + 1e-6)
    loss = tf.reduce_mean(precision * tf.square(y_true - mean) + tf.math.log(variance))
    return loss

# Build and train the model
model = build_heteroscedastic_model(X_train.shape[1])
model.compile(optimizer="adam", loss=heteroscedastic_loss)
model.fit(X_train, [y_train, y_train], epochs=50, batch_size=32, verbose=0)

# Predict mean and variance
mean_pred, variance_pred = model.predict(X_test)

print(f"Sample Mean Predictions: {mean_pred[:5].flatten()}")
print(f"Sample Variance Predictions (Uncertainty): {variance_pred[:5].flatten()}")

Summary

These methods provide robust ways to introduce and quantify uncertainty in supervised learning:
	1.	Ensemble Models: Capture epistemic uncertainty (model uncertainty).
	2.	Quantile Regression: Estimate prediction intervals for aleatoric uncertainty.
	3.	Monte Carlo Dropout: Simulate stochastic predictions.
	4.	Bootstrapping: Generate confidence intervals for predictions.
	5.	Heteroscedastic Regression: Dynamically estimate variance for aleatoric uncertainty.

Let me know if you’d like a deeper explanation or additional methods!
