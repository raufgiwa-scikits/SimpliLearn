import numpy as np


def calculate_metrics_fast(y_true, y_score, weight):
    y_true = np.asarray(y_true)
    y_score = np.asarray(y_score)
    weight = np.asarray(weight)

    # Sort descending by score
    sort_idx = np.argsort(-y_score)
    y_true = y_true[sort_idx]
    y_score = y_score[sort_idx]
    weight = weight[sort_idx]

    # Cumulative sums
    cum_weight = np.cumsum(weight)
    cum_true = np.cumsum(weight * y_true)
    cum_false = cum_weight - cum_true

    total_pos = cum_true[-1]
    total_neg = cum_false[-1]

    TPR = cum_true / total_pos
    FPR = cum_false / total_neg
    Precision = cum_true / cum_weight
    Recall = TPR

    thresholds = y_score
    KS_vals = np.abs(TPR - FPR)
    KS_stat = np.max(KS_vals)
    best_idx = np.argmax(KS_vals)
    best_thresh = thresholds[best_idx]

    # Remove duplicate thresholds (same scores)
    _, unique_idx = np.unique(thresholds, return_index=True)
    unique_idx = np.sort(unique_idx)

    fpr = FPR[unique_idx]
    tpr = TPR[unique_idx]
    precision = Precision[unique_idx]
    recall = Recall[unique_idx]
    thresholds = thresholds[unique_idx]

    # AUCs using trapezoidal rule
    roc_auc = np.trapz(tpr, fpr)
    if len(recall) > 1:
        auc_pr = np.trapz(precision[np.argsort(recall)], np.sort(recall))
    else:
        auc_pr = 0.0

    return {
        "thresholds": thresholds,
        "fpr": fpr,
        "tpr": tpr,
        "precision": precision,
        "recall": recall,
        "KS": KS_stat,
        "Best Threshold": best_thresh,
        "Best Index": best_idx,
        "ROC AUC": roc_auc,
        "AUC PR": auc_pr
    }


def calculate_fpr_tpr_precision_recall_weighted(y_true, y_score, weight):
    y_true = np.array(y_true)
    y_score = np.array(y_score)
    weight = np.array(weight)

    # Sort descending by score
    sorted_idx = np.argsort(-y_score)
    y_true = y_true[sorted_idx]
    y_score = y_score[sorted_idx]
    weight = weight[sorted_idx]

    thresholds = np.unique(y_score)[::-1]

    total_pos = np.sum(weight[y_true == 1])
    total_neg = np.sum(weight[y_true == 0])

    tpr_list, fpr_list, precision_list, recall_list = [], [], [], []

    for thresh in thresholds:
        pred = (y_score >= thresh).astype(int)

        TP = np.sum(weight[(pred == 1) & (y_true == 1)])
        FP = np.sum(weight[(pred == 1) & (y_true == 0)])
        FN = np.sum(weight[(pred == 0) & (y_true == 1)])
        TN = np.sum(weight[(pred == 0) & (y_true == 0)])

        TPR = TP / (TP + FN) if (TP + FN) > 0 else 0
        FPR = FP / (FP + TN) if (FP + TN) > 0 else 0
        Precision = TP / (TP + FP) if (TP + FP) > 0 else 1.0
        Recall = TPR

        tpr_list.append(TPR)
        fpr_list.append(FPR)
        precision_list.append(Precision)
        recall_list.append(Recall)

    return np.array(thresholds), np.array(fpr_list), np.array(tpr_list), np.array(precision_list), np.array(recall_list)



def compute_all_metrics(y_true, y_score, weight):
    thresholds, fpr, tpr, precision, recall = calculate_fpr_tpr_precision_recall_weighted(
        y_true, y_score, weight)

    # KS Statistic and Best Threshold
    ks_values = np.abs(tpr - fpr)
    ks_stat = np.max(ks_values)
    best_thresh_idx = np.argmax(ks_values)
    best_thresh = thresholds[best_thresh_idx]

    # ROC AUC (trapezoidal rule)
    roc_auc = np.trapz(tpr, fpr)

    # AUC PR (trapezoidal rule)
    sorted_idx = np.argsort(recall)
    auc_pr = np.trapz(precision[sorted_idx], recall[sorted_idx])

    return {
        "thresholds": thresholds,
        "fpr": fpr,
        "tpr": tpr,
        "precision": precision,
        "recall": recall,
        "KS": ks_stat,
        "Best Threshold": best_thresh,
        "ROC AUC": roc_auc,
        "AUC PR": auc_pr
    }


def weighted_log_loss(y_true, y_prob, weight, eps=1e-15):
    y_true = np.array(y_true)
    y_prob = np.clip(np.array(y_prob), eps, 1 - eps)
    weight = np.array(weight)

    loss = -np.sum(weight * (y_true * np.log(y_prob) + (1 - y_true) * np.log(1 - y_prob))) / np.sum(weight)
    return loss

def weighted_confusion_matrix(y_true, y_score, weight, threshold):
    y_true = np.array(y_true)
    y_score = np.array(y_score)
    weight = np.array(weight)

    pred = (y_score >= threshold).astype(int)

    TP = np.sum(weight[(pred == 1) & (y_true == 1)])
    FP = np.sum(weight[(pred == 1) & (y_true == 0)])
    FN = np.sum(weight[(pred == 0) & (y_true == 1)])
    TN = np.sum(weight[(pred == 0) & (y_true == 0)])

    return {
        "TP": TP,
        "FP": FP,
        "FN": FN,
        "TN": TN
    }
# Example Data
y_true = [0, 1, 1, 0, 1, 0, 0]
y_score = [0.1, 0.9, 0.8, 0.3, 0.75, 0.2, 0.6]
weight =  [1,   1,   2,   1,   1,    1,   1]

# Compute metrics
metrics = compute_all_metrics(y_true, y_score, weight)
logloss = weighted_log_loss(y_true, y_score, weight)
conf_matrix = weighted_confusion_matrix(y_true, y_score, weight, metrics["Best Threshold"])

# Display
print("KS Statistic:", metrics["KS"])
print("Best Threshold:", metrics["Best Threshold"])
print("ROC AUC:", metrics["ROC AUC"])
print("AUC PR:", metrics["AUC PR"])
print("Log Loss:", logloss)
print("Confusion Matrix at Best Threshold:", conf_matrix)
