Table 5.1: Score Distribution for Combined In-Time and Out-of-Time Data Period: Mar – Jul, 2024

Introduction
To further investigate the distributional behavior of model scores across various datasets, the MVO generated Table 5.1. This table is a summary of score distributions segmented by training, validation, out-of-time, in-time, and overall datasets.

Purpose of Table 5.1
Table 5.1 is a tabular representation of fraud rates, total amounts, and statistical summaries (median and standard deviation) of model scores for both fraudulent and non-fraudulent accounts. It offers insight into the concentration, variability, and effectiveness of the score outputs across distinct dataset partitions.

Technical and Statistical Review

The proportion of fraud is markedly low across all partitions, with the highest observed in the in-time set (0.0284) and the lowest in the validation set (0.0085), indicating the expected rarity of ATO fraud.

The median score for fraudulent accounts consistently surpasses that of non-fraudulent accounts. For example, in the training set, the median fraud score is 0.1031, while the non-fraud is 0.0093—indicating strong model separation.

Score standard deviations for fraud are considerably higher than for non-fraud across all splits (e.g., Train: 0.3115 vs 0.0093), suggesting greater uncertainty or variability in how fraud cases are scored.

The validation and out-of-time sets maintain similar statistical properties, hinting at temporal score stability.

The 'All' row confirms overall score behavior and validates that the model maintains separation between fraud and non-fraud across the full data scope.

Insights and Inference

A strong separation is observed in score medians between fraud and non-fraud cases, a positive indicator of discriminatory power.

The increase in standard deviation among fraudulent samples reflects diversity in detected fraudulent behavior patterns, which might merit further segmentation or clustering for profiling.

The in-time set’s slightly higher fraud rate suggests potential model overfitting to recent patterns; however, this could also reflect an actual increase in fraud incidence during that period.

Potential Problems and Risks

The very low fraud rates across sets could challenge model calibration, especially in real-time detection, leading to false positives or overlooked fraud.

The high standard deviation in fraud scores may pose consistency issues in ranking and decision-making, especially if the model is sensitive to outlier behavior.

There’s a slight difference in score medians and deviations between train and OOT sets—warranting further drift analysis to ensure long-term model stability.

Mitigation Strategies

Employ post-training calibration (e.g., Platt scaling or isotonic regression) to ensure better probability estimates.

Implement score monitoring over time to detect drift and reassess thresholds.

Use stratified retraining and frequent sampling updates to ensure better fraud representation.

Establish cutoffs based on business cost-benefit tradeoffs to manage false positives while capturing fraud cases.

